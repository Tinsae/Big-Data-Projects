{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassificationModel, RandomForestClassifier\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|spam|             message|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with spark session\n",
    "location = \"/user/edureka_672184/m8_datasets/SMSSpamCollection\"\n",
    "raw_df = spark.read.option(\"delimiter\", \"\\t\").csv(location).toDF(\"spam\", \"message\")\n",
    "\n",
    "raw_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|             message|spam|\n",
      "+--------------------+----+\n",
      "|Go until jurong p...| ham|\n",
      "|Ok lar... Joking ...| ham|\n",
      "+--------------------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with spark context\n",
    "raw = sparkContext.textFile(location) \\\n",
    ".map(lambda line: line.split(\"\\t\")) \\\n",
    ".map(lambda row: Row(spam=row[0].strip(), message=row[1].strip()))\n",
    "\n",
    "raw_df = spark.createDataFrame(raw)\n",
    "raw_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract words from the SMS message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+\n",
      "|             message|spam|               words|\n",
      "+--------------------+----+--------------------+\n",
      "|Go until jurong p...| ham|[go, until, juron...|\n",
      "|Ok lar... Joking ...| ham|[ok, lar..., joki...|\n",
      "|Free entry in 2 a...|spam|[free, entry, in,...|\n",
      "+--------------------+----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "transformed = tokenizer.transform(raw_df)\n",
    "transformed.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+--------------------+\n",
      "|             message|spam|               words|            filtered|\n",
      "+--------------------+----+--------------------+--------------------+\n",
      "|Go until jurong p...| ham|[go, until, juron...|[go, jurong, poin...|\n",
      "|Ok lar... Joking ...| ham|[ok, lar..., joki...|[ok, lar..., joki...|\n",
      "|Free entry in 2 a...|spam|[free, entry, in,...|[free, entry, 2, ...|\n",
      "+--------------------+----+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)\n",
    "cleaned.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'so', u'than', u'too', u'very', u's']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWordsRemover().getStopWords()[115:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modify the stop words to include your custom words such as ‘-‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+--------------------+\n",
      "|             message|spam|               words|            filtered|\n",
      "+--------------------+----+--------------------+--------------------+\n",
      "|Go until jurong p...| ham|[go, until, juron...|[go, jurong, poin...|\n",
      "|Ok lar... Joking ...| ham|[ok, lar..., joki...|[ok, lar..., joki...|\n",
      "|Free entry in 2 a...|spam|[free, entry, in,...|[free, entry, 2, ...|\n",
      "+--------------------+----+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove dash\n",
    "stopwords = StopWordsRemover().getStopWords() + [\"-\"]\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)\n",
    "cleaned.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create the features from SMS message using CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+--------------------+--------------------+-----+\n",
      "|             message|spam|               words|            filtered|            features|label|\n",
      "+--------------------+----+--------------------+--------------------+--------------------+-----+\n",
      "|Go until jurong p...| ham|[go, until, juron...|[go, jurong, poin...|(13498,[8,12,33,6...|  0.0|\n",
      "|Ok lar... Joking ...| ham|[ok, lar..., joki...|[ok, lar..., joki...|(13498,[0,26,308,...|  0.0|\n",
      "|Free entry in 2 a...|spam|[free, entry, in,...|[free, entry, 2, ...|(13498,[2,14,20,3...|  1.0|\n",
      "+--------------------+----+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate features\n",
    "count_vect_model = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\").fit(cleaned)\n",
    "featured = count_vect_model.transform(cleaned)\n",
    "\n",
    "# convert to binary label\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\").fit(featured)\n",
    "indexed = indexer.transform(featured)\n",
    "indexed.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Split the data into train and test - decide on a strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+--------------------+--------------------+-----+\n",
      "|             message|spam|               words|            filtered|            features|label|\n",
      "+--------------------+----+--------------------+--------------------+--------------------+-----+\n",
      "|\"AH POOR BABY!HOP...| ham|[\"ah, poor, baby!...|[\"ah, poor, baby!...|(13498,[0,2,8,69,...|  0.0|\n",
      "|\"ALRITE HUNNY!WOT...| ham|[\"alrite, hunny!w...|[\"alrite, hunny!w...|(13498,[0,2,68,21...|  0.0|\n",
      "|\"Are you comingdo...| ham|[\"are, you, comin...|[\"are, comingdown...|(13498,[3577,8197...|  0.0|\n",
      "+--------------------+----+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = indexed.randomSplit([0.7, 0.3], seed = 12345)\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use logistic regression and check the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(13498,[0,2,47,71...|  0.0|       0.0|\n",
      "|(13498,[9,15,45,5...|  0.0|       0.0|\n",
      "|(13498,[2,9,19,32...|  0.0|       0.0|\n",
      "|(13498,[3,7,287,1...|  0.0|       0.0|\n",
      "|(13498,[200,1770,...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "('Accuracy ', 0.5)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "lrModel = lr.fit(train)\n",
    "predictions = lrModel.transform(test)\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(5)\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Try to use a Random Forest classifier and see if it increases the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy', 0.515695067264574)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(10)\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Introduce bi-gram and tri-gram and note the change in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              ngrams|            features|           features2|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[\"ah poor, poor b...|(13498,[0,2,8,69,...|(37504,[173,2643,...|\n",
      "|[\"alrite hunny!wo...|(13498,[0,2,68,21...|(37504,[81,6995,7...|\n",
      "|[\"are comingdown,...|(13498,[3577,8197...|(37504,[8893,2915...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = NGram().setN(2).setInputCol(\"filtered\").setOutputCol(\"ngrams\")\n",
    "ngrams_df = ngram.transform(indexed)\n",
    "\n",
    "count_vect_model = CountVectorizer().setInputCol(\"ngrams\").setOutputCol(\"features2\").fit(ngrams_df)\n",
    "featured = count_vect_model.transform(ngrams_df)\n",
    "\n",
    "train, test = featured.randomSplit([0.7, 0.3], seed = 12345)\n",
    "train.select(\"ngrams\", \"features\", \"features2\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy', 0.5044843049327354)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features2\").setNumTrees(10)\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Decide on a strategy and generate a data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "stopwords = StopWordsRemover().getStopWords()+ [\"-\"]\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cvmodel = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\")\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\")\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "pipeline = Pipeline().setStages([tokenizer, remover, cvmodel, indexer, lr])\n",
    "model = pipeline.fit(raw_df)\n",
    "model.write().overwrite().save(\"use_cases/spam_model_ca1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline\n",
    "pipeline = pipeline = PipelineModel.load(\"use_cases/spam_model_ca1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy', 0.5044843049327354)\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = pipeline.transform(raw_df)\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
