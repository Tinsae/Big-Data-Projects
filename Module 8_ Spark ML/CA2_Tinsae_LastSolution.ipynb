{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"M8-CA2-Fraud-TGA\").getOrCreate()\n",
    "\n",
    "sparkContext = spark.sparkContext\n",
    "sqlContext = SQLContext(sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(messageid=u'Message-ID: <7257046.1075853654621.JavaMail.evans@thyme>', date=u'Fri, 16 Feb 2001 10', from_=u'stacy.dickson@enron.com', to_=u'jeffrey.hodge@enron.com', subject=u'Re: GISB'), Row(messageid=u'Message-ID: <3576232.1075858078644.JavaMail.evans@thyme>', date=u'Fri, 8 Dec 2000 08', from_=u'martin.cuilla@enron.com', to_=u's.nadalin@thehiringspot.com', subject=u'Re: new years'), Row(messageid=u'Message-ID: <23635942.1075853654829.JavaMail.evans@thyme>', date=u'Mon, 12 Mar 2001 08', from_=u'stacy.dickson@enron.com', to_=u'russell.diamond@enron.com', subject=u'RE: WEPCO')]\n",
      "772\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "\n",
    "# https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tar.gz\n",
    "\n",
    "# Extract, maildir will be created\n",
    "\n",
    "# tar -zxvf enron_mail_20150507.tar.gz\n",
    "\n",
    "# mkdir mail_dataset\n",
    "\n",
    "# cd mail_dataset\n",
    "\n",
    "# Copy sent mails from names starting with letter c and d\n",
    "\n",
    "# cp --backup=existing --suffix=.orig -r ../maildir/c*/sent*/ .\n",
    "# cp --backup=existing --suffix=.orig -r ../maildir/d*/sent*/ .\n",
    "\n",
    "\n",
    "# sed -i -e 's/\\r//g' sent/*\n",
    "\n",
    "# head -q -n 5 sent/* | paste - - - - - -d# > output.csv\n",
    "\n",
    "location = \"/user/edureka_672184/m8_datasets/maildir/output.csv\"\n",
    "raw = spark.read.option(\"delimiter\", \"#\").csv(location).toDF(\"messageid\", \"date\", \"from_\", \"to_\", \"subject\")\n",
    "# withColumn creates a new dataframe after adding the given column\n",
    "df = raw.withColumn(\"date\", F.trim(F.split(raw.date, \":\")[1])) \\\n",
    ".withColumn(\"from_\", F.trim(F.split(raw.from_, \":\")[1])) \\\n",
    ".withColumn(\"to_\", F.trim(F.split(raw.to_, \":\")[1])) \\\n",
    ".withColumn(\"subject\",F.trim(F.split(raw.subject,\"Subject:\")[1]))\n",
    "\n",
    "print(df.head(3))\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Display the top 10 high-frequency users based on weekly numbers of emails sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(messageid=u'Message-ID: <7257046.1075853654621.JavaMail.evans@thyme>', date=u'Fri, 16 Feb 2001 10', from_=u'stacy.dickson@enron.com', to_=u'jeffrey.hodge@enron.com', subject=u'Re: GISB', week=7),\n",
       " Row(messageid=u'Message-ID: <3576232.1075858078644.JavaMail.evans@thyme>', date=u'Fri, 8 Dec 2000 08', from_=u'martin.cuilla@enron.com', to_=u's.nadalin@thehiringspot.com', subject=u'Re: new years', week=49),\n",
       " Row(messageid=u'Message-ID: <23635942.1075853654829.JavaMail.evans@thyme>', date=u'Mon, 12 Mar 2001 08', from_=u'stacy.dickson@enron.com', to_=u'russell.diamond@enron.com', subject=u'RE: WEPCO', week=11)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SEE THAT WEEKS ARE CORRECTLY EXTRACTED\n",
    "df1 = df.withColumn(\"week\", F.weekofyear(F.unix_timestamp(df.date, \"EEE, dd MMM yyyy HH\").cast(\"timestamp\")))\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(max(week)=50)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET MAXIMUM WEEK\n",
    "maxweek = df1.agg(F.max(df1.week)).first()[0]\n",
    "print(maxweek)\n",
    "df1.agg(F.max(df1.week)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               from_|count|\n",
      "+--------------------+-----+\n",
      "|twanda.sweet@enro...|   27|\n",
      "|martin.cuilla@enr...|   16|\n",
      "|michelle.cash@enr...|  540|\n",
      "|stacy.dickson@enr...|  189|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"from_\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+\n",
      "|               from_|count|avgcount|\n",
      "+--------------------+-----+--------+\n",
      "|michelle.cash@enr...|  540|    10.8|\n",
      "|stacy.dickson@enr...|  189|    3.78|\n",
      "|twanda.sweet@enro...|   27|    0.54|\n",
      "|martin.cuilla@enr...|   16|    0.32|\n",
      "+--------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"from_\").count().withColumn(\"avgcount\", F.col(\"count\") / maxweek).sort(F.col(\"avgcount\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract top 20 keywords from the subject text for both\n",
    "• for the top 10 high-frequency users and\n",
    "\n",
    "• for the non-high frequency users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create tokenizer(it is a transformer)\n",
    "tokenizer = Tokenizer().setInputCol(\"subject\").setOutputCol(\"words\")\n",
    "# then transform df1\n",
    "transformed = tokenizer.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(from_=u'michelle.cash@enron.com', count=540),\n",
       " Row(from_=u'stacy.dickson@enron.com', count=189),\n",
       " Row(from_=u'twanda.sweet@enron.com', count=27),\n",
       " Row(from_=u'martin.cuilla@enron.com', count=16)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupby from and get count, sort by count descending and take top 10\n",
    "top10  = df1.groupby(\"from_\").count().sort(F.col(\"count\").desc()).take(10)\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 540, 'from_': u'michelle.cash@enron.com'}\n"
     ]
    }
   ],
   "source": [
    "# convert Row(key1=val1, key2 = val2) to Dictionary form {key1:val1, key2:val2}\n",
    "print(top10[0].asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'michelle.cash@enron.com',\n",
       " u'stacy.dickson@enron.com',\n",
       " u'twanda.sweet@enron.com',\n",
       " u'martin.cuilla@enron.com']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_users = [v.asDict()[\"from_\"] for v in top10]\n",
    "top_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737\n"
     ]
    }
   ],
   "source": [
    "# filter out top users. 737 of 772 emails are sent by these users \n",
    "topuserdata = transformed.filter(transformed.subject != \"\").filter(transformed.from_.isin(top_users))\n",
    "print(topuserdata.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|   keyword|count|\n",
      "+----------+-----+\n",
      "|       re:|  412|\n",
      "| agreement|   85|\n",
      "|         -|   51|\n",
      "|       and|   46|\n",
      "|        of|   42|\n",
      "|       for|   41|\n",
      "|     enron|   33|\n",
      "|   project|   30|\n",
      "|       fw:|   26|\n",
      "|        to|   25|\n",
      "|    master|   23|\n",
      "|      firm|   20|\n",
      "|   meeting|   20|\n",
      "|agreements|   20|\n",
      "|  contract|   19|\n",
      "|       nui|   19|\n",
      "|         &|   18|\n",
      "|employment|   18|\n",
      "|       new|   17|\n",
      "|        --|   17|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creates one row for one keyword\n",
    "# so other attributes will be repeated many times\n",
    "topuserdata.withColumn(\"keyword\", F.explode(\"words\")).groupBy(\"keyword\").count().sort(F.col(\"count\").desc()).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|keyword|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "otheruserdata = transformed.filter(transformed.subject != \"\").filter(transformed.from_.isin(top_users) == False)\n",
    "otheruserdata.withColumn(\"keyword\",F.explode(\"words\")).groupBy(\"keyword\").count().sort(F.col(\"count\").desc()).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract top 10 keywords by identifying removing the common stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|   keyword|count|\n",
      "+----------+-----+\n",
      "|       re:|  412|\n",
      "| agreement|   85|\n",
      "|         -|   51|\n",
      "|       and|   46|\n",
      "|        of|   42|\n",
      "|       for|   41|\n",
      "|     enron|   33|\n",
      "|   project|   30|\n",
      "|       fw:|   26|\n",
      "|        to|   25|\n",
      "|    master|   23|\n",
      "|   meeting|   20|\n",
      "|      firm|   20|\n",
      "|agreements|   20|\n",
      "|  contract|   19|\n",
      "|       nui|   19|\n",
      "|employment|   18|\n",
      "|         &|   18|\n",
      "|       new|   17|\n",
      "|        --|   17|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)\n",
    "cleaned.filter(cleaned.subject != \"\").withColumn(\"keyword\",F.explode(cleaned.words)).groupBy(\"keyword\").count().sort(F.col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extend the stop words dictionary by adding your own stop words such as ‘— ‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            filtered|count|\n",
      "+--------------------+-----+\n",
      "|[opportunities, ena]|   48|\n",
      "| [enron, pdi, japan]|   34|\n",
      "|[master, firm, pu...|   30|\n",
      "|[ena, deals, move...|   25|\n",
      "|[master, service,...|   24|\n",
      "|[request, submitt...|   24|\n",
      "|[recent, developm...|   22|\n",
      "|[che, club, meeti...|   22|\n",
      "|[payroll, company...|   21|\n",
      "|[b, &, j, gas, &,...|   21|\n",
      "|[data, protection...|   20|\n",
      "|[master, agreemen...|   20|\n",
      "|[section, 5.14, s...|   20|\n",
      "|[enron, investmen...|   20|\n",
      "|[triple, lutz, bi...|   20|\n",
      "|[new, vendors, co...|   19|\n",
      "|[project, triple,...|   19|\n",
      "|[cargill, energy,...|   19|\n",
      "|[georgia, pacific...|   18|\n",
      "|[mission, impossi...|   18|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopWords = StopWordsRemover().getStopWords() + [\"-\", \"re:\", \"fw:\"]\n",
    "remover = StopWordsRemover().setStopWords(stopWords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)\n",
    "# data grouped by set of filtered words\n",
    "# this shows that some collection of words occur more frequently than others\n",
    "cleaned.filter(cleaned.subject != \"\").withColumn(\"keyword\",F.explode(cleaned.words)).groupBy(\"filtered\").count().sort(F.col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Introduce a new column label to identify new, replied, and forwarded messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(msgtype=1),\n",
       " Row(msgtype=1),\n",
       " Row(msgtype=0),\n",
       " Row(msgtype=0),\n",
       " Row(msgtype=0)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = cleaned.withColumn(\"msgtype\", \n",
    "                         F.when(cleaned.subject.startswith(\"Re:\"),1). \\\n",
    "                         otherwise(F.when(cleaned.subject.startswith(\"Fw:\"),2). \\\n",
    "                         otherwise(0)))\n",
    "\n",
    "df2.select(\"msgtype\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Get the trend of the over mail activity using the pivot table from spark itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----+----+\n",
      "|week|  0|   1|   2|\n",
      "+----+---+----+----+\n",
      "|  31|  5|   2|null|\n",
      "|  34|  5|   6|null|\n",
      "|  28|  7|   3|null|\n",
      "|  26| 18|  23|null|\n",
      "|  27|  2|   1|null|\n",
      "|  44| 10|   7|null|\n",
      "|  12| 11|   9|null|\n",
      "|  22|  1|   1|null|\n",
      "|  47|  3|null|null|\n",
      "|   1|  1|null|null|\n",
      "|  13| 16|   5|null|\n",
      "|  16|  9|   7|   1|\n",
      "|   6|  3|   1|null|\n",
      "|   3|  2|   1|null|\n",
      "|  20| 11|  13|null|\n",
      "|  40|  1|   4|null|\n",
      "|  48|  1|   5|null|\n",
      "|   5|  2|null|null|\n",
      "|  19| 16|  24|null|\n",
      "|  41|  6|  16|null|\n",
      "+----+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(\"week\").pivot(\"msgtype\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Use k-means clustering to create 4 clusters from the extracted keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(messageid=u'Message-ID: <7257046.1075853654621.JavaMail.evans@thyme>', date=u'Fri, 16 Feb 2001 10', from_=u'stacy.dickson@enron.com', to_=u'jeffrey.hodge@enron.com', subject=u'Re: GISB', week=7, words=[u're:', u'gisb'], filtered=[u'gisb'], msgtype=1, features=SparseVector(942, {30: 1.0}))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert keywords to feature vector\n",
    "df4 = df2.filter(df2.subject != \"\")\n",
    "cvmodel =CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\").fit(df4)\n",
    "featured = cvmodel.transform(df4)\n",
    "featured.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[messageid: string, date: string, from_: string, to_: string, subject: string, week: int, words: array<string>, filtered: array<string>, msgtype: int, features: vector, prediction: int]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kmeans takes \"features\" columun as its feature-set without being explicitly told\n",
    "kmeans = KMeans().setK(4).setSeed(1L)\n",
    "model = kmeans.fit(featured)\n",
    "predictions = model.transform(featured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Use LDA to generate 4 topics from the extracted keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+--------------------+\n",
      "|topic|      termIndices|         termWeights|\n",
      "+-----+-----------------+--------------------+\n",
      "|    0| [11, 15, 37, 47]|[0.00576072810041...|\n",
      "|    1|    [2, 4, 8, 30]|[0.01015907726442...|\n",
      "|    2|[39, 24, 21, 116]|[0.00509658536382...|\n",
      "|    3|   [0, 3, 16, 13]|[0.02476422712367...|\n",
      "+-----+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LDA().setK(4).setMaxIter(10)\n",
    "model = lda.fit(featured)\n",
    "topics = model.describeTopics(4)\n",
    "topics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'new', u'project', u'employee', u'agreement']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_indices = topics.select(\"termIndices\").rdd.map(lambda x:x[0][0]).collect()\n",
    "[cvmodel.vocabulary[v] for v in topic_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.Can you identify top keywords in the spam messages across the organization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
