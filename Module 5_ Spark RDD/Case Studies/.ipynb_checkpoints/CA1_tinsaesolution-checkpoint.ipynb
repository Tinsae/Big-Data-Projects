{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "from operator import add, mul\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conf = SparkConf().setAppName(\"M5-CA1-TGA\")\n",
    "#sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession.builder.appName(\"M5-CA1-TGA\").getOrCreate() # singleton instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'application_1528714825862_139422'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load csv into spark as a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location = \"/user/edureka_672184/use_cases/AppleStore.csv\"\n",
    "apple = sc.textFile(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parse the data as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7197"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split by comma\n",
    "# exclude header\n",
    "import re\n",
    "pattern = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "apple_lines = apple.map(lambda line: re.split(pattern, line)).filter((lambda cols: cols[0]!='\"\"'))\n",
    "apple_lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert bytes to MB and GB in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide by 1000\n",
    "# divide by 1000000\n",
    "MB = apple_lines.map(lambda x: float(x[3]) / 1000.)\n",
    "GB = MB.map(lambda x: x / 1000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GB.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. List top 10 trending apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'\"Infinity Blade\"', u'177050'),\n",
       " (u'\"Geometry Dash Meltdown\"', u'117470'),\n",
       " (u'\"My Verizon\"', u'107245'),\n",
       " (u'\"Real Basketball\"', u'94315'),\n",
       " (u'\"Zillow Real Estate - Homes for Sale & for Rent\"', u'88478'),\n",
       " (u'\"WhatsApp Messenger\"', u'73088'),\n",
       " (u'\"Clear Vision (17+)\"', u'69225'),\n",
       " (u'\"Guess My Age \\ue020 Math Magic\"', u'68841'),\n",
       " (u'\"Trigger Fist\"', u'58269'),\n",
       " (u'\"Zappos: shop shoes & clothes, fast free shipping\"', u'39452')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id, track_name, rating_count_ver\n",
    "apple_lines.sortBy(lambda line: float(line[7]), ascending=False).map(lambda line: (line[2], line[7])).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The difference in the average number of screenshots displayed of highest and lowest rating apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3675834041\n"
     ]
    }
   ],
   "source": [
    "# \"ipadSc_urls.num\"\n",
    "\n",
    "# user rating above 3 and below 3\n",
    "\n",
    "highest = apple_lines.filter(lambda line: float(line[8]) > 3.0)\n",
    "higest_sc = highest.map(lambda line: float(line[14])).mean()\n",
    "lowest = apple_lines.filter(lambda line: float(line[8]) < 3.0)\n",
    "lowest_sc = lowest.map(lambda line: float(line[14])).mean()\n",
    "\n",
    "print(higest_sc - lowest_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What percentage of high rated apps support multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5483\n",
      "50 %\n"
     ]
    }
   ],
   "source": [
    "# lang.num\n",
    "total_higest = highest.count()\n",
    "print(total_higest)\n",
    "highest_multiple = highest.map(lambda line: float(line[15])).filter(lambda value: value > 1).count()\n",
    "print(str(highest_multiple * 100 / total_higest) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# location = \"/user/edureka_672184/use_cases/appleStore_description.csv\"\n",
    "# # df = spark.read.option(\"header\", \"true\").option('quote', '\"').option('escape', '\"').load(location)\n",
    "# # df = spark.read.csv(location, sep=',', escape='\"', header=True)\n",
    "# #goog_df = spark.read.csv(GOOG_CSV,inferSchema=True,header=True)\n",
    "# df = spark.read.csv(location, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 7. How does app details contribute to user ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 8. Compare the statistics of different app groups/genres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'\"1\"',\n",
       "  u'\"281656475\"',\n",
       "  u'\"PAC-MAN Premium\"',\n",
       "  u'100788224',\n",
       "  u'\"USD\"',\n",
       "  u'3.99',\n",
       "  u'21292',\n",
       "  u'26',\n",
       "  u'4',\n",
       "  u'4.5',\n",
       "  u'\"6.3.5\"',\n",
       "  u'\"4+\"',\n",
       "  u'\"Games\"',\n",
       "  u'38',\n",
       "  u'5',\n",
       "  u'10',\n",
       "  u'1'],\n",
       " [u'\"2\"',\n",
       "  u'\"281796108\"',\n",
       "  u'\"Evernote - stay organized\"',\n",
       "  u'158578688',\n",
       "  u'\"USD\"',\n",
       "  u'0',\n",
       "  u'161065',\n",
       "  u'26',\n",
       "  u'4',\n",
       "  u'3.5',\n",
       "  u'\"8.2.2\"',\n",
       "  u'\"4+\"',\n",
       "  u'\"Productivity\"',\n",
       "  u'37',\n",
       "  u'5',\n",
       "  u'23',\n",
       "  u'1'],\n",
       " [u'\"3\"',\n",
       "  u'\"281940292\"',\n",
       "  u'\"WeatherBug - Local Weather, Radar, Maps, Alerts\"',\n",
       "  u'100524032',\n",
       "  u'\"USD\"',\n",
       "  u'0',\n",
       "  u'188583',\n",
       "  u'2822',\n",
       "  u'3.5',\n",
       "  u'4.5',\n",
       "  u'\"5.0.0\"',\n",
       "  u'\"4+\"',\n",
       "  u'\"Weather\"',\n",
       "  u'37',\n",
       "  u'5',\n",
       "  u'3',\n",
       "  u'1']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_lines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'\"Travel\"', 1.1203703703703702), (u'\"Food & Drink\"', 1.552380952380952), (u'\"Photo & Video\"', 1.4732951289398313), (u'\"Music\"', 4.835434782608699), (u'\"Reference\"', 4.836875000000004), (u'\"Business\"', 5.116315789473686), (u'\"Navigation\"', 4.1247826086956545), (u'\"Lifestyle\"', 0.8854166666666657), (u'\"Social Networking\"', 0.3398802395209582), (u'\"Productivity\"', 4.330561797752813)]\n"
     ]
    }
   ],
   "source": [
    "genre_prices = apple_lines.map(lambda line: (line[12], float(line[5])))\n",
    "grp = genre_prices.groupByKey()\n",
    "# show mean app price by genere\n",
    "print(list((j[0], sum(list(j[1]))/len(list(j[1])) ) for j in grp.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Does length of app description contribute to the ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"/user/edureka_672184/use_cases/appleStore_description.csv\"\n",
    "description = sc.textFile(location)\n",
    "pattern = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "#pattern = '(?:((?:\"(?:[^\"]*\"\")*[^\"]*\"(?:,|$))|(?:[^\",]*(?:,|$)))|(.+))?'\n",
    "# split by pattern and ignore header\n",
    "description = description.map(lambda line: line.split(pattern))\n",
    "# remove white space\n",
    "description = description.filter(lambda cols: cols[0] != '')\n",
    "# remove header\n",
    "description = description.filter(lambda cols: 'id' not in cols[0])\n",
    "\n",
    "current_id = 0\n",
    "def wordCountByID(alist):\n",
    "    # get global variable\n",
    "    global current_id\n",
    "    # split each line by comma\n",
    "    elements = alist[0].split(\",\")\n",
    "    # remove double quotes\n",
    "    idd = elements[0].replace('\"', \"\")\n",
    "    # if first element is a digit\n",
    "    # it is an application id\n",
    "    if(idd.isdigit()):\n",
    "        # convert it to int\n",
    "        current_id = int(idd)\n",
    "        # return nothing\n",
    "        return []\n",
    "    # it is a list full of text\n",
    "    else:\n",
    "        # associate words with the current id\n",
    "        return [[(current_id, (word, 1)) for word in statement.split()] for statement in elements]\n",
    "# get pairs like this (id, (word,1)    \n",
    "description_wList = description.map(wordCountByID).filter(lambda listt: len(listt) != 0)\n",
    "# flatten list of lists and group by key\n",
    "word_freq = description_wList.flatMap(lambda listt: listt[0]).groupByKey()\n",
    "# find length of each iteratable per group the sort it by that length in ascending order, then select top 10\n",
    "top10 = word_freq.map(lambda j: (j[0], len(list(j[1])))).sortBy(lambda line: line[1], ascending=False).take(10)\n",
    "# find least 10 \n",
    "least10 = word_freq.map(lambda j: (j[0], len(list(j[1])))).sortBy(lambda line: line[1], ascending=True).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10ids = [i[0] for i in top10]\n",
    "least10ids = [i[0] for i in least10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[976257847,\n",
       " 391965015,\n",
       " 289446241,\n",
       " 519952689,\n",
       " 1065719308,\n",
       " 1167873588,\n",
       " 1185580782,\n",
       " 1086688310,\n",
       " 982936366,\n",
       " 395893124]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least10ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'3', u'2', u'3', u'1.5', u'0', u'4', u'0', u'0', u'0', u'4']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get ratings of top 10\n",
    "def manipulate1(x):\n",
    "    # column one is id\n",
    "    # 1:-1 ignores the first and last quotations\n",
    "    if int(x[1][1:-1]) in top10ids:\n",
    "        # column 8 is overall rating\n",
    "        return x[8]\n",
    "apple_lines.map(manipulate).filter(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'3', u'2', u'3', u'1.5', u'0', u'4', u'0', u'0', u'0', u'4']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get ratings of least 10\n",
    "def manipulate2(x):\n",
    "    if int(x[1][1:-1]) in least10ids:\n",
    "        return x[8]\n",
    "apple_lines.map(manipulate).filter(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the top 10 highly described apps have a rating about 4. Apps with small description are rated below 4, including 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.1.0.cloudera2-1.cdh5.7.0.p0.171658/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).\n",
      "WARNING: Running spark-class from user-defined location.\n",
      "19/07/22 06:34:20 INFO spark.SparkContext: Running Spark version 2.1.0.cloudera2\n",
      "19/07/22 06:34:20 INFO spark.SecurityManager: Changing view acls to: edureka_672184\n",
      "19/07/22 06:34:20 INFO spark.SecurityManager: Changing modify acls to: edureka_672184\n",
      "19/07/22 06:34:20 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "19/07/22 06:34:20 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "19/07/22 06:34:20 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edureka_672184); groups with view permissions: Set(); users  with modify permissions: Set(edureka_672184); groups with modify permissions: Set()\n",
      "19/07/22 06:34:21 INFO util.Utils: Successfully started service 'sparkDriver' on port 41120.\n",
      "19/07/22 06:34:21 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "19/07/22 06:34:21 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "19/07/22 06:34:22 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "19/07/22 06:34:22 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "19/07/22 06:34:22 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-ec0ad8bd-6a2c-43e1-abcb-478615d88bb6\n",
      "19/07/22 06:34:22 INFO memory.MemoryStore: MemoryStore started with capacity 93.3 MB\n",
      "19/07/22 06:34:22 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "19/07/22 06:34:23 INFO yarn.Client: Requesting a new application from cluster with 4 NodeManagers\n",
      "19/07/22 06:34:23 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (4096 MB per container)\n",
      "19/07/22 06:34:23 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "19/07/22 06:34:23 INFO yarn.Client: Setting up container launch context for our AM\n",
      "19/07/22 06:34:23 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "19/07/22 06:34:24 INFO yarn.Client: Preparing resources for our AM container\n",
      "19/07/22 06:34:25 INFO yarn.Client: Uploading resource file:/tmp/spark-bff11b08-75d5-472d-bcac-efbceeb56dd6/__spark_conf__8524516243584765190.zip -> hdfs://nameservice1/user/edureka_672184/.sparkStaging/application_1528714825862_139495/__spark_conf__.zip\n",
      "19/07/22 06:34:26 INFO spark.SecurityManager: Changing view acls to: edureka_672184\n",
      "19/07/22 06:34:26 INFO spark.SecurityManager: Changing modify acls to: edureka_672184\n",
      "19/07/22 06:34:26 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "19/07/22 06:34:26 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "19/07/22 06:34:26 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edureka_672184); groups with view permissions: Set(); users  with modify permissions: Set(edureka_672184); groups with modify permissions: Set()\n",
      "19/07/22 06:34:26 INFO yarn.Client: Submitting application application_1528714825862_139495 to ResourceManager\n",
      "19/07/22 06:34:26 INFO impl.YarnClientImpl: Submitted application application_1528714825862_139495\n",
      "19/07/22 06:34:26 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1528714825862_139495 and attemptId None\n",
      "19/07/22 06:34:27 INFO yarn.Client: Application report for application_1528714825862_139495 (state: ACCEPTED)\n",
      "19/07/22 06:34:27 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: root.default\n",
      "\t start time: 1563777266299\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-20-0-21-161.ec2.internal:8088/proxy/application_1528714825862_139495/\n",
      "\t user: edureka_672184\n",
      "19/07/22 06:34:28 INFO yarn.Client: Application report for application_1528714825862_139495 (state: ACCEPTED)\n",
      "19/07/22 06:34:29 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)\n",
      "19/07/22 06:34:29 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-20-0-21-161.ec2.internal,ip-20-0-21-196.ec2.internal, PROXY_URI_BASES -> http://ip-20-0-21-161.ec2.internal:8088/proxy/application_1528714825862_139495,http://ip-20-0-21-196.ec2.internal:8088/proxy/application_1528714825862_139495), /proxy/application_1528714825862_139495\n",
      "19/07/22 06:34:29 INFO yarn.Client: Application report for application_1528714825862_139495 (state: ACCEPTED)\n",
      "19/07/22 06:34:30 INFO yarn.Client: Application report for application_1528714825862_139495 (state: RUNNING)\n",
      "19/07/22 06:34:30 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 20.0.31.210\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: root.default\n",
      "\t start time: 1563777266299\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-20-0-21-161.ec2.internal:8088/proxy/application_1528714825862_139495/\n",
      "\t user: edureka_672184\n",
      "19/07/22 06:34:30 INFO cluster.YarnClientSchedulerBackend: Application application_1528714825862_139495 has started running.\n",
      "19/07/22 06:34:30 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41578.\n",
      "19/07/22 06:34:30 INFO netty.NettyBlockTransferService: Server created on 20.0.32.93:41578\n",
      "19/07/22 06:34:30 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "19/07/22 06:34:30 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 20.0.32.93, 41578, None)\n",
      "19/07/22 06:34:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager 20.0.32.93:41578 with 93.3 MB RAM, BlockManagerId(driver, 20.0.32.93, 41578, None)\n",
      "19/07/22 06:34:30 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 20.0.32.93, 41578, None)\n",
      "19/07/22 06:34:30 INFO storage.BlockManager: external shuffle service port = 7337\n",
      "19/07/22 06:34:30 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 20.0.32.93, 41578, None)\n",
      "19/07/22 06:34:30 INFO util.log: Logging initialized @25336ms\n",
      "19/07/22 06:34:30 INFO scheduler.EventLoggingListener: Logging events to hdfs://nameservice1/user/spark/applicationHistory/application_1528714825862_139495\n",
      "19/07/22 06:34:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (20.0.31.210:39860) with ID 2\n",
      "19/07/22 06:34:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager ip-20-0-31-210.ec2.internal:44289 with 366.3 MB RAM, BlockManagerId(2, ip-20-0-31-210.ec2.internal, 44289, None)\n",
      "19/07/22 06:34:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (20.0.31.210:39858) with ID 1\n",
      "19/07/22 06:34:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager ip-20-0-31-210.ec2.internal:45510 with 366.3 MB RAM, BlockManagerId(1, ip-20-0-31-210.ec2.internal, 45510, None)\n",
      "19/07/22 06:34:33 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "19/07/22 06:34:34 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 305.5 KB, free 93.0 MB)\n",
      "19/07/22 06:34:34 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 93.0 MB)\n",
      "19/07/22 06:34:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 20.0.32.93:41578 (size: 27.2 KB, free: 93.3 MB)\n",
      "19/07/22 06:34:34 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "19/07/22 06:34:34 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 305.5 KB, free 92.7 MB)\n",
      "19/07/22 06:34:34 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 27.2 KB, free 92.7 MB)\n",
      "19/07/22 06:34:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 20.0.32.93:41578 (size: 27.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:34 INFO spark.SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/07/22 06:34:34 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/07/22 06:34:35 INFO spark.SparkContext: Starting job: sortBy at /mnt/home/edureka_672184/m5q10.py:72\n",
      "19/07/22 06:34:35 INFO scheduler.DAGScheduler: Registering RDD 5 (groupByKey at /mnt/home/edureka_672184/m5q10.py:70)\n",
      "19/07/22 06:34:35 INFO scheduler.DAGScheduler: Got job 0 (sortBy at /mnt/home/edureka_672184/m5q10.py:72) with 2 output partitions\n",
      "19/07/22 06:34:35 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (sortBy at /mnt/home/edureka_672184/m5q10.py:72)\n",
      "19/07/22 06:34:35 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "19/07/22 06:34:35 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "19/07/22 06:34:35 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[5] at groupByKey at /mnt/home/edureka_672184/m5q10.py:70), which has no missing parents\n",
      "19/07/22 06:34:35 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.7 KB, free 92.6 MB)\n",
      "19/07/22 06:34:35 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.0 KB, free 92.6 MB)\n",
      "19/07/22 06:34:35 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 20.0.32.93:41578 (size: 7.0 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:35 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:991\n",
      "19/07/22 06:34:35 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[5] at groupByKey at /mnt/home/edureka_672184/m5q10.py:70)\n",
      "19/07/22 06:34:35 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks\n",
      "19/07/22 06:34:36 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, ip-20-0-31-210.ec2.internal, executor 1, partition 0, NODE_LOCAL, 6092 bytes)\n",
      "19/07/22 06:34:36 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, ip-20-0-31-210.ec2.internal, executor 2, partition 1, NODE_LOCAL, 6092 bytes)\n",
      "19/07/22 06:34:36 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 7.0 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:36 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 7.0 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:36 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 27.2 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:36 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 27.2 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11575 ms on ip-20-0-31-210.ec2.internal (executor 1) (1/2)\n",
      "19/07/22 06:34:47 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 11554 ms on ip-20-0-31-210.ec2.internal (executor 2) (2/2)\n",
      "19/07/22 06:34:47 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "19/07/22 06:34:47 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (groupByKey at /mnt/home/edureka_672184/m5q10.py:70) finished in 11.660 s\n",
      "19/07/22 06:34:47 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "19/07/22 06:34:47 INFO scheduler.DAGScheduler: running: Set()\n",
      "19/07/22 06:34:47 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)\n",
      "19/07/22 06:34:47 INFO scheduler.DAGScheduler: failed: Set()\n",
      "19/07/22 06:34:47 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[8] at sortBy at /mnt/home/edureka_672184/m5q10.py:72), which has no missing parents\n",
      "19/07/22 06:34:47 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 9.2 KB, free 92.6 MB)\n",
      "19/07/22 06:34:47 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.2 KB, free 92.6 MB)\n",
      "19/07/22 06:34:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 20.0.32.93:41578 (size: 5.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:47 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:991\n",
      "19/07/22 06:34:47 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[8] at sortBy at /mnt/home/edureka_672184/m5q10.py:72)\n",
      "19/07/22 06:34:47 INFO cluster.YarnScheduler: Adding task set 1.0 with 2 tasks\n",
      "19/07/22 06:34:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, ip-20-0-31-210.ec2.internal, executor 1, partition 0, NODE_LOCAL, 5829 bytes)\n",
      "19/07/22 06:34:47 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, ip-20-0-31-210.ec2.internal, executor 2, partition 1, NODE_LOCAL, 5829 bytes)\n",
      "19/07/22 06:34:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 5.2 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:47 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 5.2 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:47 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 20.0.31.210:39858\n",
      "19/07/22 06:34:47 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 20.0.31.210:39860\n",
      "19/07/22 06:34:47 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 173 bytes\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 308 ms on ip-20-0-31-210.ec2.internal (executor 1) (1/2)\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 310 ms on ip-20-0-31-210.ec2.internal (executor 2) (2/2)\n",
      "19/07/22 06:34:48 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: ResultStage 1 (sortBy at /mnt/home/edureka_672184/m5q10.py:72) finished in 0.313 s\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Job 0 finished: sortBy at /mnt/home/edureka_672184/m5q10.py:72, took 13.199027 s\n",
      "19/07/22 06:34:48 INFO spark.SparkContext: Starting job: sortBy at /mnt/home/edureka_672184/m5q10.py:72\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Got job 1 (sortBy at /mnt/home/edureka_672184/m5q10.py:72) with 2 output partitions\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (sortBy at /mnt/home/edureka_672184/m5q10.py:72)\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (PythonRDD[9] at sortBy at /mnt/home/edureka_672184/m5q10.py:72), which has no missing parents\n",
      "19/07/22 06:34:48 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 8.9 KB, free 92.6 MB)\n",
      "19/07/22 06:34:48 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.2 KB, free 92.6 MB)\n",
      "19/07/22 06:34:48 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 20.0.32.93:41578 (size: 5.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:48 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:991\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[9] at sortBy at /mnt/home/edureka_672184/m5q10.py:72)\n",
      "19/07/22 06:34:48 INFO cluster.YarnScheduler: Adding task set 3.0 with 2 tasks\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, ip-20-0-31-210.ec2.internal, executor 2, partition 0, NODE_LOCAL, 5830 bytes)\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5, ip-20-0-31-210.ec2.internal, executor 1, partition 1, NODE_LOCAL, 5830 bytes)\n",
      "19/07/22 06:34:48 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 5.2 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:48 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 5.2 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 236 ms on ip-20-0-31-210.ec2.internal (executor 2) (1/2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 246 ms on ip-20-0-31-210.ec2.internal (executor 1) (2/2)\n",
      "19/07/22 06:34:48 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: ResultStage 3 (sortBy at /mnt/home/edureka_672184/m5q10.py:72) finished in 0.248 s\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Job 1 finished: sortBy at /mnt/home/edureka_672184/m5q10.py:72, took 0.271565 s\n",
      "19/07/22 06:34:48 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:441\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Registering RDD 11 (sortBy at /mnt/home/edureka_672184/m5q10.py:72)\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Got job 2 (runJob at PythonRDD.scala:441) with 1 output partitions\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (runJob at PythonRDD.scala:441)\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 5)\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (PairwiseRDD[11] at sortBy at /mnt/home/edureka_672184/m5q10.py:72), which has no missing parents\n",
      "19/07/22 06:34:48 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.9 KB, free 92.6 MB)\n",
      "19/07/22 06:34:48 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.8 KB, free 92.6 MB)\n",
      "19/07/22 06:34:48 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 20.0.32.93:41578 (size: 5.8 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:48 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:991\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (PairwiseRDD[11] at sortBy at /mnt/home/edureka_672184/m5q10.py:72)\n",
      "19/07/22 06:34:48 INFO cluster.YarnScheduler: Adding task set 5.0 with 2 tasks\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, ip-20-0-31-210.ec2.internal, executor 1, partition 0, NODE_LOCAL, 5668 bytes)\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, ip-20-0-31-210.ec2.internal, executor 2, partition 1, NODE_LOCAL, 5668 bytes)\n",
      "19/07/22 06:34:48 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 5.8 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:48 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 5.8 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 255 ms on ip-20-0-31-210.ec2.internal (executor 1) (1/2)\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 277 ms on ip-20-0-31-210.ec2.internal (executor 2) (2/2)\n",
      "19/07/22 06:34:48 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (sortBy at /mnt/home/edureka_672184/m5q10.py:72) finished in 0.278 s\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: running: Set()\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 6)\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: failed: Set()\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (PythonRDD[14] at RDD at PythonRDD.scala:48), which has no missing parents\n",
      "19/07/22 06:34:48 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.2 KB, free 92.6 MB)\n",
      "19/07/22 06:34:48 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.2 KB, free 92.6 MB)\n",
      "19/07/22 06:34:48 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 20.0.32.93:41578 (size: 4.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:48 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:991\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (PythonRDD[14] at RDD at PythonRDD.scala:48)\n",
      "19/07/22 06:34:48 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, ip-20-0-31-210.ec2.internal, executor 2, partition 0, NODE_LOCAL, 5679 bytes)\n",
      "19/07/22 06:34:48 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 4.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:48 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 20.0.31.210:39860\n",
      "19/07/22 06:34:48 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 170 bytes\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 26 ms on ip-20-0-31-210.ec2.internal (executor 2) (1/1)\n",
      "19/07/22 06:34:48 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: ResultStage 6 (runJob at PythonRDD.scala:441) finished in 0.027 s\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:441, took 0.320558 s\n",
      "19/07/22 06:34:48 INFO spark.SparkContext: Starting job: sortBy at /mnt/home/edureka_672184/m5q10.py:74\n",
      "19/07/22 06:34:48 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 173 bytes\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Got job 3 (sortBy at /mnt/home/edureka_672184/m5q10.py:74) with 2 output partitions\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (sortBy at /mnt/home/edureka_672184/m5q10.py:74)\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (PythonRDD[15] at sortBy at /mnt/home/edureka_672184/m5q10.py:74), which has no missing parents\n",
      "19/07/22 06:34:48 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.2 KB, free 92.6 MB)\n",
      "19/07/22 06:34:48 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.2 KB, free 92.6 MB)\n",
      "19/07/22 06:34:48 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 20.0.32.93:41578 (size: 5.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:48 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:991\n",
      "19/07/22 06:34:48 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (PythonRDD[15] at sortBy at /mnt/home/edureka_672184/m5q10.py:74)\n",
      "19/07/22 06:34:48 INFO cluster.YarnScheduler: Adding task set 8.0 with 2 tasks\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 9, ip-20-0-31-210.ec2.internal, executor 1, partition 0, NODE_LOCAL, 5830 bytes)\n",
      "19/07/22 06:34:48 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 10, ip-20-0-31-210.ec2.internal, executor 2, partition 1, NODE_LOCAL, 5830 bytes)\n",
      "19/07/22 06:34:48 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 5.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:48 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 5.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:48 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 20.0.31.210:39860\n",
      "19/07/22 06:34:48 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 20.0.31.210:39858\n",
      "19/07/22 06:34:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 9) in 231 ms on ip-20-0-31-210.ec2.internal (executor 1) (1/2)\n",
      "19/07/22 06:34:49 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 10) in 238 ms on ip-20-0-31-210.ec2.internal (executor 2) (2/2)\n",
      "19/07/22 06:34:49 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: ResultStage 8 (sortBy at /mnt/home/edureka_672184/m5q10.py:74) finished in 0.240 s\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Job 3 finished: sortBy at /mnt/home/edureka_672184/m5q10.py:74, took 0.246603 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/07/22 06:34:49 INFO spark.SparkContext: Starting job: sortBy at /mnt/home/edureka_672184/m5q10.py:74\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Got job 4 (sortBy at /mnt/home/edureka_672184/m5q10.py:74) with 2 output partitions\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (sortBy at /mnt/home/edureka_672184/m5q10.py:74)\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (PythonRDD[16] at sortBy at /mnt/home/edureka_672184/m5q10.py:74), which has no missing parents\n",
      "19/07/22 06:34:49 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 8.9 KB, free 92.6 MB)\n",
      "19/07/22 06:34:49 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.2 KB, free 92.5 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 20.0.32.93:41578 (size: 5.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:49 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:991\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (PythonRDD[16] at sortBy at /mnt/home/edureka_672184/m5q10.py:74)\n",
      "19/07/22 06:34:49 INFO cluster.YarnScheduler: Adding task set 10.0 with 2 tasks\n",
      "19/07/22 06:34:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 11, ip-20-0-31-210.ec2.internal, executor 2, partition 0, NODE_LOCAL, 5830 bytes)\n",
      "19/07/22 06:34:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 12, ip-20-0-31-210.ec2.internal, executor 1, partition 1, NODE_LOCAL, 5830 bytes)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 5.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:49 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 12) in 226 ms on ip-20-0-31-210.ec2.internal (executor 1) (1/2)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 5.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on ip-20-0-31-210.ec2.internal:44289 in memory (size: 5.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on ip-20-0-31-210.ec2.internal:45510 in memory (size: 5.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 20.0.32.93:41578 in memory (size: 5.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 11) in 673 ms on ip-20-0-31-210.ec2.internal (executor 2) (2/2)\n",
      "19/07/22 06:34:49 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: ResultStage 10 (sortBy at /mnt/home/edureka_672184/m5q10.py:74) finished in 0.674 s\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Job 4 finished: sortBy at /mnt/home/edureka_672184/m5q10.py:74, took 0.694533 s\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 20.0.32.93:41578 in memory (size: 5.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on ip-20-0-31-210.ec2.internal:44289 in memory (size: 5.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on ip-20-0-31-210.ec2.internal:45510 in memory (size: 5.2 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 20.0.32.93:41578 in memory (size: 4.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on ip-20-0-31-210.ec2.internal:44289 in memory (size: 4.2 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 20.0.32.93:41578 in memory (size: 5.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on ip-20-0-31-210.ec2.internal:44289 in memory (size: 5.2 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on ip-20-0-31-210.ec2.internal:45510 in memory (size: 5.2 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 20.0.32.93:41578 in memory (size: 5.8 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on ip-20-0-31-210.ec2.internal:44289 in memory (size: 5.8 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on ip-20-0-31-210.ec2.internal:45510 in memory (size: 5.8 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:49 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:441\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Registering RDD 18 (sortBy at /mnt/home/edureka_672184/m5q10.py:74)\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Got job 5 (runJob at PythonRDD.scala:441) with 1 output partitions\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (runJob at PythonRDD.scala:441)\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 12)\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 12 (PairwiseRDD[18] at sortBy at /mnt/home/edureka_672184/m5q10.py:74), which has no missing parents\n",
      "19/07/22 06:34:49 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.9 KB, free 92.6 MB)\n",
      "19/07/22 06:34:49 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.8 KB, free 92.6 MB)\n",
      "19/07/22 06:34:49 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 20.0.32.93:41578 (size: 5.8 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:49 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:991\n",
      "19/07/22 06:34:49 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (PairwiseRDD[18] at sortBy at /mnt/home/edureka_672184/m5q10.py:74)\n",
      "19/07/22 06:34:49 INFO cluster.YarnScheduler: Adding task set 12.0 with 2 tasks\n",
      "19/07/22 06:34:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 13, ip-20-0-31-210.ec2.internal, executor 2, partition 0, NODE_LOCAL, 5668 bytes)\n",
      "19/07/22 06:34:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 14, ip-20-0-31-210.ec2.internal, executor 1, partition 1, NODE_LOCAL, 5668 bytes)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 5.8 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 5.8 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 13) in 239 ms on ip-20-0-31-210.ec2.internal (executor 2) (1/2)\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 14) in 244 ms on ip-20-0-31-210.ec2.internal (executor 1) (2/2)\n",
      "19/07/22 06:34:50 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: ShuffleMapStage 12 (sortBy at /mnt/home/edureka_672184/m5q10.py:74) finished in 0.246 s\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: running: Set()\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 13)\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: failed: Set()\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (PythonRDD[21] at RDD at PythonRDD.scala:48), which has no missing parents\n",
      "19/07/22 06:34:50 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.2 KB, free 92.6 MB)\n",
      "19/07/22 06:34:50 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.2 KB, free 92.6 MB)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 20.0.32.93:41578 (size: 4.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:50 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:991\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (PythonRDD[21] at RDD at PythonRDD.scala:48)\n",
      "19/07/22 06:34:50 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 15, ip-20-0-31-210.ec2.internal, executor 1, partition 0, NODE_LOCAL, 5679 bytes)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 4.2 KB, free: 366.3 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/07/22 06:34:50 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 20.0.31.210:39858\n",
      "19/07/22 06:34:50 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 170 bytes\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 15) in 26 ms on ip-20-0-31-210.ec2.internal (executor 1) (1/1)\n",
      "19/07/22 06:34:50 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: ResultStage 13 (runJob at PythonRDD.scala:441) finished in 0.026 s\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Job 5 finished: runJob at PythonRDD.scala:441, took 0.284706 s\n",
      "19/07/22 06:34:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/07/22 06:34:50 INFO spark.SparkContext: Starting job: collect at /mnt/home/edureka_672184/m5q10.py:79\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Got job 6 (collect at /mnt/home/edureka_672184/m5q10.py:79) with 2 output partitions\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (collect at /mnt/home/edureka_672184/m5q10.py:79)\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (PythonRDD[22] at collect at /mnt/home/edureka_672184/m5q10.py:79), which has no missing parents\n",
      "19/07/22 06:34:50 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 7.0 KB, free 92.6 MB)\n",
      "19/07/22 06:34:50 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.2 KB, free 92.6 MB)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 20.0.32.93:41578 (size: 4.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:50 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:991\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 14 (PythonRDD[22] at collect at /mnt/home/edureka_672184/m5q10.py:79)\n",
      "19/07/22 06:34:50 INFO cluster.YarnScheduler: Adding task set 14.0 with 2 tasks\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 16, ip-20-0-31-210.ec2.internal, executor 2, partition 0, NODE_LOCAL, 6093 bytes)\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 17, ip-20-0-31-210.ec2.internal, executor 1, partition 1, NODE_LOCAL, 6093 bytes)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 4.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 4.2 KB, free: 366.3 MB)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 27.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 27.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 17) in 123 ms on ip-20-0-31-210.ec2.internal (executor 1) (1/2)\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 16) in 141 ms on ip-20-0-31-210.ec2.internal (executor 2) (2/2)\n",
      "19/07/22 06:34:50 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: ResultStage 14 (collect at /mnt/home/edureka_672184/m5q10.py:79) finished in 0.141 s\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Job 6 finished: collect at /mnt/home/edureka_672184/m5q10.py:79, took 0.147898 s\n",
      "19/07/22 06:34:50 INFO spark.SparkContext: Starting job: collect at /mnt/home/edureka_672184/m5q10.py:80\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Got job 7 (collect at /mnt/home/edureka_672184/m5q10.py:80) with 2 output partitions\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Final stage: ResultStage 15 (collect at /mnt/home/edureka_672184/m5q10.py:80)\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Submitting ResultStage 15 (PythonRDD[23] at collect at /mnt/home/edureka_672184/m5q10.py:80), which has no missing parents\n",
      "19/07/22 06:34:50 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 7.0 KB, free 92.6 MB)\n",
      "19/07/22 06:34:50 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.2 KB, free 92.6 MB)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 20.0.32.93:41578 (size: 4.2 KB, free: 93.2 MB)\n",
      "19/07/22 06:34:50 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:991\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 15 (PythonRDD[23] at collect at /mnt/home/edureka_672184/m5q10.py:80)\n",
      "19/07/22 06:34:50 INFO cluster.YarnScheduler: Adding task set 15.0 with 2 tasks\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 18, ip-20-0-31-210.ec2.internal, executor 2, partition 0, NODE_LOCAL, 6093 bytes)\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 19, ip-20-0-31-210.ec2.internal, executor 1, partition 1, NODE_LOCAL, 6093 bytes)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on ip-20-0-31-210.ec2.internal:45510 (size: 4.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:50 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on ip-20-0-31-210.ec2.internal:44289 (size: 4.2 KB, free: 366.2 MB)\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 19) in 76 ms on ip-20-0-31-210.ec2.internal (executor 1) (1/2)\n",
      "19/07/22 06:34:50 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 18) in 90 ms on ip-20-0-31-210.ec2.internal (executor 2) (2/2)\n",
      "19/07/22 06:34:50 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: ResultStage 15 (collect at /mnt/home/edureka_672184/m5q10.py:80) finished in 0.091 s\n",
      "19/07/22 06:34:50 INFO scheduler.DAGScheduler: Job 7 finished: collect at /mnt/home/edureka_672184/m5q10.py:80, took 0.096865 s\n",
      "Showing ratings of highly described apps\n",
      "4\n",
      "4.5\n",
      "5\n",
      "4\n",
      "5\n",
      "0\n",
      "4.5\n",
      "0\n",
      "0\n",
      "4\n",
      "()\n",
      "Showing ratings of least described apps\n",
      "3\n",
      "2\n",
      "3\n",
      "1.5\n",
      "0\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "()\n",
      "19/07/22 06:34:50 INFO spark.SparkContext: Invoking stop() from shutdown hook\n",
      "19/07/22 06:34:51 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\n",
      "19/07/22 06:34:51 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\n",
      "19/07/22 06:34:51 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n",
      "19/07/22 06:34:51 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\n",
      "(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\n",
      "19/07/22 06:34:51 INFO cluster.YarnClientSchedulerBackend: Stopped\n",
      "19/07/22 06:34:51 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "19/07/22 06:34:51 INFO memory.MemoryStore: MemoryStore cleared\n",
      "19/07/22 06:34:51 INFO storage.BlockManager: BlockManager stopped\n",
      "19/07/22 06:34:51 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "19/07/22 06:34:51 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "19/07/22 06:34:51 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "19/07/22 06:34:51 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "19/07/22 06:34:51 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-bff11b08-75d5-472d-bcac-efbceeb56dd6/pyspark-e4d926d5-e7ee-42ce-aaab-80be05d6f3fc\n",
      "19/07/22 06:34:51 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-bff11b08-75d5-472d-bcac-efbceeb56dd6\n"
     ]
    }
   ],
   "source": [
    "# 10. Create a spark-submit application for the same and print the findings in the log\n",
    "!spark2-submit /mnt/home/edureka_672184/m5q10.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
