{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "from operator import add, mul\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"M5-CA2-ApacheLogs-TGA\").getOrCreate() # singleton instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load file as a text file in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input.csv into HDFS\n",
    "# !hdfs dfs -mkdir use_cases/Logs \n",
    "# !hdfs dfs -put access.clean.log  use_cases/Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-agent}i\\\n",
    "\n",
    "\n",
    "127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif HTTP/1.0\" 200 2326 \"http://www.example.com/start.html\" \"Mozilla/4.08 [en] (Win98; I ;Nav)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 127.0.0.1 (%h): This is the IP address of the client (remote host) which made the request to the server. \n",
    "- (%l): The \"hyphen\" in the output indicates that the requested piece of information is not available. In this case, the information that is not available is the RFC 1413 identity of the client determined by identd on the clients machine. \n",
    "- frank (%u) - This is the userid of the person requesting the document as determined by HTTP authentication. If the document is not password protected, this entry will be \"-\" just like the previous one.\n",
    "- [10/Oct/2000:13:55:36 -0700] (%t): The time that the server finished processing the request. The format is:[day/month/year:hour:minute:second zone]\n",
    "- day = 2*digit, month = 3*letter, year = 4*digit, hour = 2*digit, minute = 2*digit, second = 2*digit,zone = (`+' | `-') 4*digit\n",
    "\n",
    "- \"GET /apache_pb.gif HTTP/1.0\" (\\\"%r\\\"): The request line from the client is given in double quotes. The request line contains a great deal of useful information. First, the method used by the client is GET. Second, the client requested the resource /apache_pb.gif, and third, the client used the protocol HTTP/1.0. It is also possible to log one or more parts of the request line independently. \n",
    "- 200 (%>s) This is the status code that the server sends back to the client. This information is very valuable, because it reveals whether the request resulted in a successful response (codes beginning in 2), a redirection (codes beginning in 3), an error caused by the client (codes beginning in 4), or an error in the server (codes beginning in 5). The full list of possible status codes can be found in the HTTP specification (RFC2616 section 10).\n",
    "- 2326 (%b): The last entry indicates the size of the object returned to the client, not including the response headers. If no content was returned to the client, this value will be \"-\". To log \"0\" for no content, use %B instead.\n",
    "- \"http://www.example.com/start.html\" (\\\"%{Referer}i\\\")- The \"Referer\" (sic) HTTP request header. This gives the site that the client reports having been referred from. (This should be the page that links to or includes /apache_pb.gif).\n",
    "- \"Mozilla/4.08 [en] (Win98; I ;Nav)\" (\\\"%{User-agent}i\\\")- The User-Agent HTTP request header. This is the identifying information that the client browser reports about itself.\n",
    "\n",
    "Reference\n",
    "https://httpd.apache.org/docs/1.3/logs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code\n",
    "# import re\n",
    "# pattern = r'^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] (\\S+)\\s?(\\S+)?\\s?(\\S+)? (\\d{3}|-) (\\d+|-)\\s?([^\"]*)\\s?\"?([^\"]*)?$'\n",
    "# logline = '109.169.248.247 - - [12/Dec/2015:18:25:11 +0100] POST /administrator/index.php HTTP/1.1 200 4494 http://almhuette-raith.at/administrator/ Mozilla/5.0 (Windows NT 6.0; rv:34.0) Gecko/20100101 Firefox/34.0'\n",
    "# matchh = re.search(pattern, logline)\n",
    "\n",
    "# for g in range(1, 12):\n",
    "#     print(matchh.group(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"/user/edureka_672184/use_cases/Logs/access.clean.log\"\n",
    "# load spark dataframe\n",
    "raw = spark.sparkContext.textFile(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.types import Row\n",
    "# Returns  a  dictionary  containing  the  parts  of  the  Apache Access  Log.\n",
    "def parse_apache_log_line(logline):\n",
    "    pattern = r'^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] (\\S+)\\s?(\\S+)?\\s?(\\S+)? (\\d{3}|-) (\\d+|-)\\s?([^\"]*)\\s?\"?([^\"]*)?$'\n",
    "    match =  re.search(pattern,  logline)\n",
    "    if  match  is None:\n",
    "        return  Row(\n",
    "        ipAddress = '0.0.0.0',\n",
    "        clientIdentd = '-',\n",
    "        userId = '-',\n",
    "        dateTime = '01/Jan/1970:00:00:00',\n",
    "        dateTimeZone = '+0100',\n",
    "        method  = 'GET',\n",
    "        endpoint = 'Unknown',\n",
    "        protocol = 'HTTP/1.1',\n",
    "        responseCode =  200,\n",
    "        contentSize  =  '1000',\n",
    "        referer = 'Unknown'\n",
    "    ) \n",
    "    else:\n",
    "        g10 = match.group(10).strip(\"- \").strip(\" -\").strip()\n",
    "        if(g10.startswith(\"http\")):\n",
    "            referer_ = g10.split()[0]\n",
    "        else:\n",
    "            referer_ = \"-\"\n",
    "            \n",
    "        return  Row(\n",
    "            ipAddress = match.group(1),\n",
    "            clientIdentd = match.group(2),\n",
    "            userId = match.group(3),\n",
    "            dateTime = match.group(4).split()[0],\n",
    "            dateTimeZone = match.group(4).split()[1],\n",
    "            method  = match.group(5),\n",
    "            endpoint = match.group(6),\n",
    "            protocol = match.group(7),\n",
    "            responseCode =  int(match.group(8)),\n",
    "            contentSize  =  match.group(9),\n",
    "            referer = referer_\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------------+------------+--------------------+---------------+------+--------+--------------------+------------+------+\n",
      "|clientIdentd|contentSize|            dateTime|dateTimeZone|            endpoint|      ipAddress|method|protocol|             referer|responseCode|userId|\n",
      "+------------+-----------+--------------------+------------+--------------------+---------------+------+--------+--------------------+------------+------+\n",
      "|           -|       4263|12/Dec/2015:18:25:11|       +0100|     /administrator/|109.169.248.247|   GET|HTTP/1.1|                   -|         200|     -|\n",
      "|           -|       4494|12/Dec/2015:18:25:11|       +0100|/administrator/in...|109.169.248.247|  POST|HTTP/1.1|http://almhuette-...|         200|     -|\n",
      "|           -|       4263|12/Dec/2015:18:31:08|       +0100|     /administrator/|    46.72.177.4|   GET|HTTP/1.1|                   -|         200|     -|\n",
      "|           -|       4494|12/Dec/2015:18:31:08|       +0100|/administrator/in...|    46.72.177.4|  POST|HTTP/1.1|http://almhuette-...|         200|     -|\n",
      "|           -|       4263|12/Dec/2015:18:31:25|       +0100|     /administrator/| 83.167.113.100|   GET|HTTP/1.1|                   -|         200|     -|\n",
      "|           -|       4494|12/Dec/2015:18:31:25|       +0100|/administrator/in...| 83.167.113.100|  POST|HTTP/1.1|http://almhuette-...|         200|     -|\n",
      "|           -|       4263|12/Dec/2015:18:32:10|       +0100|     /administrator/|   95.29.198.15|   GET|HTTP/1.1|                   -|         200|     -|\n",
      "|           -|       4494|12/Dec/2015:18:32:11|       +0100|/administrator/in...|   95.29.198.15|  POST|HTTP/1.1|http://almhuette-...|         200|     -|\n",
      "|           -|       4263|12/Dec/2015:18:32:56|       +0100|     /administrator/|  109.184.11.34|   GET|HTTP/1.1|                   -|         200|     -|\n",
      "|           -|       4494|12/Dec/2015:18:32:56|       +0100|/administrator/in...|  109.184.11.34|  POST|HTTP/1.1|http://almhuette-...|         200|     -|\n",
      "|           -|       4263|12/Dec/2015:18:33:51|       +0100|     /administrator/|   91.227.29.79|   GET|HTTP/1.1|                   -|         200|     -|\n",
      "|           -|       4494|12/Dec/2015:18:33:52|       +0100|/administrator/in...|   91.227.29.79|  POST|HTTP/1.1|http://almhuette-...|         200|     -|\n",
      "|           -|       4263|12/Dec/2015:18:36:16|       +0100|     /administrator/|  90.154.66.233|   GET|HTTP/1.1|                   -|         200|     -|\n",
      "|           -|       4494|12/Dec/2015:18:36:16|       +0100|/administrator/in...|  90.154.66.233|  POST|HTTP/1.1|http://almhuette-...|         200|     -|\n",
      "|           -|       4263|12/Dec/2015:18:38:42|       +0100|     /administrator/|  95.140.24.131|   GET|HTTP/1.1|                   -|         200|     -|\n",
      "|           -|       4494|12/Dec/2015:18:38:42|       +0100|/administrator/in...|  95.140.24.131|  POST|HTTP/1.1|http://almhuette-...|         200|     -|\n",
      "|           -|       4263|12/Dec/2015:18:38:55|       +0100|     /administrator/|  95.188.245.16|   GET|HTTP/1.1|                   -|         200|     -|\n",
      "|           -|       4494|12/Dec/2015:18:38:56|       +0100|/administrator/in...|  95.188.245.16|  POST|HTTP/1.1|http://almhuette-...|         200|     -|\n",
      "|           -|       4263|12/Dec/2015:18:39:27|       +0100|     /administrator/|  46.72.213.133|   GET|HTTP/1.1|                   -|         200|     -|\n",
      "|           -|       4494|12/Dec/2015:18:39:27|       +0100|/administrator/in...|  46.72.213.133|  POST|HTTP/1.1|http://almhuette-...|         200|     -|\n",
      "+------------+-----------+--------------------+------------+--------------------+---------------+------+--------+--------------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df = raw.map(parse_apache_log_line).cache().toDF()\n",
    "logs_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find out how many 404 HTTP codes are in access logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227089\n"
     ]
    }
   ],
   "source": [
    "http404 = logs_df.filter(logs_df[\"responseCode\"] == 404)\n",
    "print(http404.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find out which URLs are broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------+\n",
      "|referer                                                                                            |\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "|http://almhuette-raith.at/administrator/components/com_sef/views/logger/config.php                 |\n",
      "|http://almhuette-raith.at/administrator/components/com_banners/web-infor.php                       |\n",
      "|http://almhuette-raith.at/components/com_content/views/featured/tmpl/config.php                    |\n",
      "|http://almhuette-raith.at/layouts/system_info.php.suspected                                        |\n",
      "|http://almhuette-raith.at/images/legacy.cms.php.suspected                                          |\n",
      "|http://almhuette-raith.at/_private/logo_img.php.suspected                                          |\n",
      "|http://almhuette-raith.at/tmp/headers_img.php                                                      |\n",
      "|http://almhuette-raith.at/assets/fckeditor/editor/filemanager/connectors/uploadtest.html           |\n",
      "|http://almhuette-raith.at/administrator/components/com_admin/controllers/update.php                |\n",
      "|http://almhuette-raith.at/templates/ot_photographer/admin/colorchooser/data.php                    |\n",
      "|http://almhuette-raith.at/media/plg_quickicon_extensionupdate/wp-xmlrpc.php                        |\n",
      "|http://almhuette-raith.at/modules/mod_virtuemart/paypal_api.php                                    |\n",
      "|http://almhuette-raith.at/plugins/captcha/jproicaptcha/websystem.php                               |\n",
      "|http://almhuette-raith.at/plugins/system/banip.php                                                 |\n",
      "|http://www.almhuette-raith.at/index.php?option=com_content&amp;view=article&amp;id=46&amp;Itemid=54|\n",
      "|http://almhuette-raith.at/index.php?option=com_content&view=article&id=50%27A=0&Itemid=56          |\n",
      "|http://almhuette-raith.at/modules/mod_apigoogle/mod_apigoogle.php                                  |\n",
      "|http://almhuette-raith.at/manager/fckeditor/editor/filemanager/connectors/uploadtest.html          |\n",
      "|http://almhuette-raith.at/modules/mod_searchmenu/ir.php                                            |\n",
      "|http://almhuette-raith.at/cache/strange.php                                                        |\n",
      "+---------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(http404.select(\"referer\").distinct().show(20, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Verify there are no null columns in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I already checked null columns when parsing the apache log file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Replace null values with constants such as 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a row with this values when null is encountered\n",
    "\n",
    "ipAddress = '0.0.0.0',\n",
    "clientIdentd = '-',\n",
    "userId = '-',\n",
    "dateTime = '01/Jan/1970:00:00:00 +0100',\n",
    "method  = 'GET',\n",
    "endpoint = 'Unknown',\n",
    "protocol = 'HTTP/1.1',\n",
    "responseCode =  200,\n",
    "contentSize  =  '1000',\n",
    "referer = 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('There are ', 31, ' nulls in the data')\n"
     ]
    }
   ],
   "source": [
    "nulls = logs_df.filter((logs_df[\"ipAddress\"] == '0.0.0.0') & (logs_df[\"referer\"] == 'Unknown'))\n",
    "print(\"There are \", nulls.count(), \" nulls in the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Parse timestamp to readable date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|datetime            |\n",
      "+--------------------+\n",
      "|12/Dec/2015:18:25:11|\n",
      "|12/Dec/2015:18:25:11|\n",
      "|12/Dec/2015:18:31:08|\n",
      "|12/Dec/2015:18:31:08|\n",
      "|12/Dec/2015:18:31:25|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.select(\"datetime\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference https://stackoverflow.com/questions/39088473/pyspark-dataframe-convert-unusual-string-format-to-timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|dateTimeParsed       |\n",
      "+---------------------+\n",
      "|2015-12-12 18:25:11.0|\n",
      "|2015-12-12 18:25:11.0|\n",
      "|2015-12-12 18:31:08.0|\n",
      "|2015-12-12 18:31:08.0|\n",
      "|2015-12-12 18:31:25.0|\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "logs_df2 = logs_df.withColumn(\"dateTimeParsed\", unix_timestamp(\"datetime\", \"dd/MMM/yyyy:hh:mm:ss\").cast(\"double\").cast(\"timestamp\"))\n",
    "logs_df2.select(\"dateTimeParsed\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Describe which HTTP status values appear in data and how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|responseCode|  count|\n",
      "+------------+-------+\n",
      "|         412|     19|\n",
      "|         400|     23|\n",
      "|         406|     53|\n",
      "|         405|     83|\n",
      "|         401|    135|\n",
      "|         501|    143|\n",
      "|         303|    247|\n",
      "|         301|    619|\n",
      "|         403|   2222|\n",
      "|         500|   3252|\n",
      "|         304|   6330|\n",
      "|         404| 227089|\n",
      "|         206| 939929|\n",
      "|         200|1157862|\n",
      "+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df2.groupBy(\"responseCode\").count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Display as chart the above stat in chart in Zeppelin notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We didn't learn Zeepelin and it is not installed in our envirioment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. How many unique hosts are there in the entire log and their average request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|      ipAddress|count|\n",
      "+---------------+-----+\n",
      "|     45.61.46.3|    1|\n",
      "|139.162.150.131|    1|\n",
      "|     5.83.104.2|    1|\n",
      "|  192.3.195.106|    1|\n",
      "| 179.215.122.32|    1|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df2.groupBy(\"ipAddress\").count().orderBy('count').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 10.Create a spark-submit application for the same and print the findings in the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.1.0.cloudera2-1.cdh5.7.0.p0.171658/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).\n",
      "WARNING: Running spark-class from user-defined location.\n",
      "19/07/23 07:19:38 INFO spark.SparkContext: Running Spark version 2.1.0.cloudera2\n",
      "19/07/23 07:19:39 INFO spark.SecurityManager: Changing view acls to: edureka_672184\n",
      "19/07/23 07:19:39 INFO spark.SecurityManager: Changing modify acls to: edureka_672184\n",
      "19/07/23 07:19:39 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "19/07/23 07:19:39 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "19/07/23 07:19:39 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edureka_672184); groups with view permissions: Set(); users  with modify permissions: Set(edureka_672184); groups with modify permissions: Set()\n",
      "19/07/23 07:19:40 INFO util.Utils: Successfully started service 'sparkDriver' on port 46740.\n",
      "19/07/23 07:19:40 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "19/07/23 07:19:40 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "19/07/23 07:19:40 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "19/07/23 07:19:40 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "19/07/23 07:19:40 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-c65c4f39-a8d4-4ea6-b4e0-31996466fc24\n",
      "19/07/23 07:19:40 INFO memory.MemoryStore: MemoryStore started with capacity 93.3 MB\n",
      "19/07/23 07:19:40 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "19/07/23 07:19:44 INFO yarn.Client: Requesting a new application from cluster with 3 NodeManagers\n",
      "19/07/23 07:19:44 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (4096 MB per container)\n",
      "19/07/23 07:19:44 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "19/07/23 07:19:44 INFO yarn.Client: Setting up container launch context for our AM\n",
      "19/07/23 07:19:44 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "19/07/23 07:19:44 INFO yarn.Client: Preparing resources for our AM container\n",
      "19/07/23 07:19:46 INFO yarn.Client: Uploading resource file:/tmp/spark-92d17402-4c2c-4361-9cd1-eec8fbe6dfb7/__spark_conf__9149946052437472545.zip -> hdfs://nameservice1/user/edureka_672184/.sparkStaging/application_1528714825862_139734/__spark_conf__.zip\n",
      "19/07/23 07:19:46 INFO spark.SecurityManager: Changing view acls to: edureka_672184\n",
      "19/07/23 07:19:46 INFO spark.SecurityManager: Changing modify acls to: edureka_672184\n",
      "19/07/23 07:19:46 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "19/07/23 07:19:46 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "19/07/23 07:19:46 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edureka_672184); groups with view permissions: Set(); users  with modify permissions: Set(edureka_672184); groups with modify permissions: Set()\n",
      "19/07/23 07:19:46 INFO yarn.Client: Submitting application application_1528714825862_139734 to ResourceManager\n",
      "19/07/23 07:19:46 INFO impl.YarnClientImpl: Submitted application application_1528714825862_139734\n",
      "19/07/23 07:19:46 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1528714825862_139734 and attemptId None\n",
      "19/07/23 07:19:48 INFO yarn.Client: Application report for application_1528714825862_139734 (state: ACCEPTED)\n",
      "19/07/23 07:19:48 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: root.default\n",
      "\t start time: 1563866386974\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-20-0-21-161.ec2.internal:8088/proxy/application_1528714825862_139734/\n",
      "\t user: edureka_672184\n",
      "19/07/23 07:19:49 INFO yarn.Client: Application report for application_1528714825862_139734 (state: ACCEPTED)\n",
      "19/07/23 07:19:49 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)\n",
      "19/07/23 07:19:49 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-20-0-21-161.ec2.internal,ip-20-0-21-196.ec2.internal, PROXY_URI_BASES -> http://ip-20-0-21-161.ec2.internal:8088/proxy/application_1528714825862_139734,http://ip-20-0-21-196.ec2.internal:8088/proxy/application_1528714825862_139734), /proxy/application_1528714825862_139734\n",
      "19/07/23 07:19:50 INFO yarn.Client: Application report for application_1528714825862_139734 (state: RUNNING)\n",
      "19/07/23 07:19:50 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 20.0.31.210\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: root.default\n",
      "\t start time: 1563866386974\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-20-0-21-161.ec2.internal:8088/proxy/application_1528714825862_139734/\n",
      "\t user: edureka_672184\n",
      "19/07/23 07:19:50 INFO cluster.YarnClientSchedulerBackend: Application application_1528714825862_139734 has started running.\n",
      "19/07/23 07:19:50 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34822.\n",
      "19/07/23 07:19:50 INFO netty.NettyBlockTransferService: Server created on 20.0.41.62:34822\n",
      "19/07/23 07:19:50 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "19/07/23 07:19:50 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 20.0.41.62, 34822, None)\n",
      "19/07/23 07:19:50 INFO storage.BlockManagerMasterEndpoint: Registering block manager 20.0.41.62:34822 with 93.3 MB RAM, BlockManagerId(driver, 20.0.41.62, 34822, None)\n",
      "19/07/23 07:19:50 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 20.0.41.62, 34822, None)\n",
      "19/07/23 07:19:50 INFO storage.BlockManager: external shuffle service port = 7337\n",
      "19/07/23 07:19:50 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 20.0.41.62, 34822, None)\n",
      "19/07/23 07:19:50 INFO util.log: Logging initialized @19240ms\n",
      "19/07/23 07:19:51 INFO scheduler.EventLoggingListener: Logging events to hdfs://nameservice1/user/spark/applicationHistory/application_1528714825862_139734\n",
      "19/07/23 07:19:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (20.0.31.210:41822) with ID 1\n",
      "19/07/23 07:19:52 INFO storage.BlockManagerMasterEndpoint: Registering block manager ip-20-0-31-210.ec2.internal:44741 with 366.3 MB RAM, BlockManagerId(1, ip-20-0-31-210.ec2.internal, 44741, None)\n",
      "19/07/23 07:19:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (20.0.31.210:41824) with ID 2\n",
      "19/07/23 07:19:53 INFO storage.BlockManagerMasterEndpoint: Registering block manager ip-20-0-31-210.ec2.internal:33647 with 366.3 MB RAM, BlockManagerId(2, ip-20-0-31-210.ec2.internal, 33647, None)\n",
      "19/07/23 07:19:53 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "19/07/23 07:19:53 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').\n",
      "19/07/23 07:19:53 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/07/23 07:19:53 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 1.1.0 using file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-logging-1.1.3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-exec-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-exec.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-jdbc-1.1.0-cdh5.11.1-standalone.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-jdbc-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-jdbc-standalone.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-jdbc.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-metastore-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-metastore.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-serde-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-serde.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-service-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-service.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/libfb303-0.9.3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/libthrift-0.9.3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/log4j-1.2.16.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hbase-client.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hbase-common.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hbase-hadoop-compat.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hbase-hadoop2-compat.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hbase-protocol.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hbase-server.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/htrace-core.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/ST4-4.0.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/accumulo-core-1.6.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/accumulo-fate-1.6.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/accumulo-start-1.6.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/accumulo-trace-1.6.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/activation-1.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/ant-1.9.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/ant-launcher-1.9.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/antlr-2.7.7.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/antlr-runtime-3.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/apache-log4j-extras-1.2.17.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/asm-3.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/asm-commons-3.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/asm-tree-3.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/avro.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/bonecp-0.8.0.RELEASE.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/calcite-avatica-1.0.0-incubating.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/calcite-core-1.0.0-incubating.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/calcite-linq4j-1.0.0-incubating.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-beanutils-1.9.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-beanutils-core-1.8.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-cli-1.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-codec-1.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-collections-3.2.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-compiler-2.7.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-compress-1.4.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-configuration-1.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-dbcp-1.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-digester-1.8.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-el-1.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-httpclient-3.0.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-io-2.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-lang-2.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-lang3-3.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-math-2.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-pool-1.5.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/commons-vfs2-2.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/curator-client-2.6.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/curator-framework-2.6.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/curator-recipes-2.6.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/datanucleus-api-jdo-3.2.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/datanucleus-core-3.2.10.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/datanucleus-rdbms-3.2.9.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/derby-10.11.1.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/eigenbase-properties-1.1.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/findbugs-annotations-1.3.9-1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/geronimo-annotation_1.0_spec-1.1.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/geronimo-jaspic_1.0_spec-1.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/geronimo-jta_1.1_spec-1.1.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/groovy-all-2.4.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/gson-2.2.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/guava-14.0.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hamcrest-core-1.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hbase-annotations.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/high-scale-lib-1.1.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-accumulo-handler-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-accumulo-handler.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-ant-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-ant.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-beeline-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-beeline.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-cli-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-cli.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-common-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-common.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-contrib-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-contrib.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-hbase-handler-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-hbase-handler.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-hwi-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-hwi.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-shims-0.23-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-shims-0.23.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-shims-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-shims-common-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-shims-common.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-shims-scheduler-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-shims-scheduler.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-shims.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-testutils.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jamon-runtime-2.3.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jackson-xc-1.9.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jackson-databind-2.2.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jackson-annotations-2.2.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/zookeeper.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/velocity-1.5.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/snappy-java-1.0.4.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/plexus-utils-1.5.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/paranamer-2.3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/oro-2.0.8.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/httpclient-4.2.5.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/xz-1.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/tempus-fugit-1.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/super-csv-2.2.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/stax-api-1.0.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/servlet-api-2.5.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/opencsv-2.3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/metrics-jvm-3.0.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/metrics-json-3.0.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/metrics-core-3.0.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/maven-scm-provider-svnexe-1.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/maven-scm-provider-svn-commons-1.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/maven-scm-api-1.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/mail-1.4.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/logredactor-1.0.3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/junit-4.11.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jta-1.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jsr305-3.0.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jsp-api-2.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jpam-1.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/joda-time-1.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jline-2.12.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jetty-all-server-7.6.0.v20120127.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jetty-all-7.6.0.v20120127.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jersey-servlet-1.14.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jersey-server-1.14.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jdo-api-3.0.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jcommander-1.32.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jasper-runtime-5.5.23.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jasper-compiler-5.5.23.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/janino-2.7.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jackson-jaxrs-1.9.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/jackson-core-2.2.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/ivy-2.0.0-rc2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/parquet-hadoop-bundle.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/stringtemplate-3.2.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/regexp-1.3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/httpcore-4.2.5.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/../hive/lib/hive-testutils-1.1.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/activation-1.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/activation.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/apacheds-i18n-2.0.0-M15.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/apacheds-i18n.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/apacheds-kerberos-codec-2.0.0-M15.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/apacheds-kerberos-codec.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/api-asn1-api-1.0.0-M20.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/api-asn1-api.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/api-util-1.0.0-M20.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/api-util.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/avro.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/aws-java-sdk-core-1.10.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/aws-java-sdk-core.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/aws-java-sdk-dynamodb-1.10.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/aws-java-sdk-dynamodb.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/aws-java-sdk-kms-1.10.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/aws-java-sdk-kms.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/aws-java-sdk-s3-1.10.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/aws-java-sdk-s3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/aws-java-sdk-sts-1.10.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/aws-java-sdk-sts.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/azure-data-lake-store-sdk-2.1.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/azure-data-lake-store-sdk.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-beanutils-1.9.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-beanutils-core-1.8.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-beanutils-core.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-beanutils.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-cli-1.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-cli.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-codec-1.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-codec.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-collections-3.2.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-collections.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-compress-1.4.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-compress.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-configuration-1.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-configuration.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-digester-1.8.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-digester.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-httpclient-3.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-httpclient.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-io-2.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-io.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-lang-2.6.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-lang.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-logging-1.1.3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-logging.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-math3-3.1.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-math3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-net-3.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/commons-net.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/curator-client-2.7.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/curator-client.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/curator-framework-2.7.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/curator-framework.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/curator-recipes-2.7.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/curator-recipes.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/gson-2.2.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/gson.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/guava-11.0.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/guava.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-annotations-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-annotations.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-auth-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-auth.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-aws-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-aws.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-azure-datalake-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-azure-datalake.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-common-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-common.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-hdfs-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-hdfs.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-mapreduce-client-app-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-mapreduce-client-app.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-mapreduce-client-common-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-mapreduce-client-common.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-mapreduce-client-core-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-mapreduce-client-core.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-mapreduce-client-jobclient.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-mapreduce-client-shuffle.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-yarn-api-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-yarn-api.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-yarn-client-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-yarn-client.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-yarn-common-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-yarn-common.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-yarn-server-common-2.6.0-cdh5.11.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/hadoop-yarn-server-common.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/htrace-core4-4.0.1-incubating.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/htrace-core4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/httpclient-4.2.5.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/httpclient.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/httpcore-4.2.5.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/httpcore.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jackson-annotations-2.2.3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jackson-core.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jackson-jaxrs-1.8.8.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jackson-databind-2.2.3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jackson-annotations.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/zookeeper.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/xz.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/xz-1.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/xmlenc.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/xmlenc-0.52.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/xml-apis.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/xml-apis-1.3.04.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/xercesImpl.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/xercesImpl-2.9.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/stax-api.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/stax-api-1.0-2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/snappy-java.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/snappy-java-1.0.4.1.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/slf4j-log4j12.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/slf4j-api.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/slf4j-api-1.7.5.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/servlet-api.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/servlet-api-2.5.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/protobuf-java.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/protobuf-java-2.5.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/paranamer.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/paranamer-2.3.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/netty.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/netty-3.10.5.Final.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/log4j.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/log4j-1.2.17.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/leveldbjni-all.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/leveldbjni-all-1.8.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jsr305.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jsr305-3.0.0.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jetty-util.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jetty-util-6.1.26.cloudera.4.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jersey-core.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jersey-core-1.9.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jersey-client.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jersey-client-1.9.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jaxb-api.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jaxb-api-2.2.2.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jackson-xc.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jackson-xc-1.8.8.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jackson-jaxrs.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jackson-databind.jar:file:/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/client/jackson-core-2.2.3.jar\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/07/23 07:19:55 INFO session.SessionState: Created local directory: /tmp/881edcc5-7354-4eae-b129-7459f1616338_resources\n",
      "19/07/23 07:19:55 INFO session.SessionState: Created HDFS directory: /tmp/hive/edureka_672184/881edcc5-7354-4eae-b129-7459f1616338\n",
      "19/07/23 07:19:55 INFO session.SessionState: Created local directory: /tmp/edureka_672184/881edcc5-7354-4eae-b129-7459f1616338\n",
      "19/07/23 07:19:55 INFO session.SessionState: Created HDFS directory: /tmp/hive/edureka_672184/881edcc5-7354-4eae-b129-7459f1616338/_tmp_space.db\n",
      "19/07/23 07:19:55 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.\n",
      "19/07/23 07:19:55 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.1.0) is /user/hive/warehouse\n",
      "19/07/23 07:19:55 INFO hive.metastore: Trying to connect to metastore with URI thrift://ip-20-0-21-161.ec2.internal:9083\n",
      "19/07/23 07:19:55 INFO hive.metastore: Opened a connection to metastore, current connections: 1\n",
      "19/07/23 07:19:55 INFO hive.metastore: Connected to metastore.\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function userdate UnixtimeToDate\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function lowercasefunc hivePkg.LowerCase\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function edu_lower hiveudf.udf\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function lower hive_udf.hive_udf_function\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function low hive_udf.Lower\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function low hive_udf.Lower\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function hj hive_pack.Lower\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function hj hive_pack.Lower\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function bup Sampleudf.Lower\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function upperconvert hive.Udf\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function upperconvert hive.Udf\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function lower_udf hive_udf.Hive_udf\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function low10 udf_hive.Lower\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function edu hiveUdf.HiveClass\n",
      "19/07/23 07:19:56 INFO metadata.Hive: Registering function low hivelower.Lower\n",
      "19/07/23 07:19:56 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 318.1 KB, free 93.0 MB)\n",
      "19/07/23 07:19:57 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.1 KB, free 93.0 MB)\n",
      "19/07/23 07:19:57 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 20.0.41.62:34822 (size: 29.1 KB, free: 93.3 MB)\n",
      "19/07/23 07:19:57 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "19/07/23 07:20:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/07/23 07:20:06 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:441\n",
      "19/07/23 07:20:06 INFO scheduler.DAGScheduler: Got job 0 (runJob at PythonRDD.scala:441) with 1 output partitions\n",
      "19/07/23 07:20:06 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:441)\n",
      "19/07/23 07:20:06 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "19/07/23 07:20:06 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "19/07/23 07:20:06 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[3] at RDD at PythonRDD.scala:48), which has no missing parents\n",
      "19/07/23 07:20:06 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.2 KB, free 93.0 MB)\n",
      "19/07/23 07:20:06 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.9 KB, free 92.9 MB)\n",
      "19/07/23 07:20:06 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 20.0.41.62:34822 (size: 4.9 KB, free: 93.3 MB)\n",
      "19/07/23 07:20:06 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:991\n",
      "19/07/23 07:20:06 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[3] at RDD at PythonRDD.scala:48)\n",
      "19/07/23 07:20:06 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\n",
      "19/07/23 07:20:06 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, ip-20-0-31-210.ec2.internal, executor 1, partition 0, NODE_LOCAL, 5948 bytes)\n",
      "19/07/23 07:20:07 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-20-0-31-210.ec2.internal:44741 (size: 4.9 KB, free: 366.3 MB)\n",
      "19/07/23 07:20:07 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-20-0-31-210.ec2.internal:44741 (size: 29.1 KB, free: 366.3 MB)\n",
      "19/07/23 07:20:26 INFO storage.BlockManagerInfo: Added rdd_2_0 in memory on ip-20-0-31-210.ec2.internal:44741 (size: 47.7 MB, free: 318.6 MB)\n",
      "19/07/23 07:20:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 19659 ms on ip-20-0-31-210.ec2.internal (executor 1) (1/1)\n",
      "19/07/23 07:20:26 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "19/07/23 07:20:26 INFO scheduler.DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:441) finished in 19.667 s\n",
      "19/07/23 07:20:26 INFO scheduler.DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:441, took 20.301869 s\n",
      "19/07/23 07:20:27 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 20.0.41.62:34822 in memory (size: 4.9 KB, free: 93.3 MB)\n",
      "19/07/23 07:20:27 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on ip-20-0-31-210.ec2.internal:44741 in memory (size: 4.9 KB, free: 318.6 MB)\n",
      "19/07/23 07:20:29 INFO codegen.CodeGenerator: Code generated in 1060.629875 ms\n",
      "19/07/23 07:20:29 INFO codegen.CodeGenerator: Code generated in 9.705363 ms\n",
      "19/07/23 07:20:29 INFO spark.SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "19/07/23 07:20:29 INFO scheduler.DAGScheduler: Registering RDD 10 (count at NativeMethodAccessorImpl.java:0)\n",
      "19/07/23 07:20:29 INFO scheduler.DAGScheduler: Got job 1 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "19/07/23 07:20:29 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (count at NativeMethodAccessorImpl.java:0)\n",
      "19/07/23 07:20:29 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "19/07/23 07:20:29 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
      "19/07/23 07:20:29 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "19/07/23 07:20:29 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 21.5 KB, free 92.9 MB)\n",
      "19/07/23 07:20:29 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 11.1 KB, free 92.9 MB)\n",
      "19/07/23 07:20:29 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 20.0.41.62:34822 (size: 11.1 KB, free: 93.3 MB)\n",
      "19/07/23 07:20:29 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:991\n",
      "19/07/23 07:20:29 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0)\n",
      "19/07/23 07:20:29 INFO cluster.YarnScheduler: Adding task set 1.0 with 4 tasks\n",
      "19/07/23 07:20:29 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, ip-20-0-31-210.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 6051 bytes)\n",
      "19/07/23 07:20:29 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, ip-20-0-31-210.ec2.internal, executor 2, partition 1, NODE_LOCAL, 6051 bytes)\n",
      "19/07/23 07:20:29 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on ip-20-0-31-210.ec2.internal:44741 (size: 11.1 KB, free: 318.6 MB)\n",
      "19/07/23 07:20:29 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on ip-20-0-31-210.ec2.internal:33647 (size: 11.1 KB, free: 366.3 MB)\n",
      "19/07/23 07:20:30 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-20-0-31-210.ec2.internal:33647 (size: 29.1 KB, free: 366.3 MB)\n",
      "19/07/23 07:20:37 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, ip-20-0-31-210.ec2.internal, executor 1, partition 2, NODE_LOCAL, 6051 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/07/23 07:20:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 7639 ms on ip-20-0-31-210.ec2.internal (executor 1) (1/4)\n",
      "19/07/23 07:20:48 INFO storage.BlockManagerInfo: Added rdd_2_1 in memory on ip-20-0-31-210.ec2.internal:33647 (size: 46.4 MB, free: 319.8 MB)\n",
      "19/07/23 07:20:50 INFO storage.BlockManagerInfo: Added rdd_2_2 in memory on ip-20-0-31-210.ec2.internal:44741 (size: 36.5 MB, free: 282.1 MB)\n",
      "19/07/23 07:20:55 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, ip-20-0-31-210.ec2.internal, executor 2, partition 3, NODE_LOCAL, 6051 bytes)\n",
      "19/07/23 07:20:55 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 26229 ms on ip-20-0-31-210.ec2.internal (executor 2) (2/4)\n",
      "19/07/23 07:20:56 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 19044 ms on ip-20-0-31-210.ec2.internal (executor 1) (3/4)\n",
      "19/07/23 07:21:05 INFO storage.BlockManagerInfo: Added rdd_2_3 in memory on ip-20-0-31-210.ec2.internal:33647 (size: 28.4 MB, free: 291.4 MB)\n"
     ]
    }
   ],
   "source": [
    "!spark2-submit /mnt/home/edureka_672184/m5ca2q10.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
